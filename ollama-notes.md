# Ollama 學習筆記

## 常用指令

| 指令 | 說明 |
|------|------|
| `ollama list` | 列出本機所有模型 |
| `ollama ps` | 查看目前執行中的模型（含記憶體佔用） |
| `ollama run <model>` | 執行模型進入對話 |
| `ollama pull <model>` | 下載模型 |
| `ollama rm <model>` | 刪除模型 |
| `ollama show <model>` | 顯示模型詳細資訊（含 Modelfile） |
| `ollama cp <src> <dst>` | 複製模型 |
| `ollama create <name> -f <Modelfile>` | 從 Modelfile 建立模型 |

### 對話中指令

| 指令 | 說明 |
|------|------|
| `/save <name>` | 儲存目前狀態為新模型 |
| `/show info` | 顯示模型資訊 |
| `/set parameter <name> <value>` | 臨時修改參數 |
| `/clear` | 清除對話歷史 |
| `/bye` | 離開對話 |

---

## Modelfile（模型設定檔）

用於定義自訂模型，可包含 system prompt、參數設定等。

### 基本結構

```dockerfile
# 指定基底模型
FROM llama3

# 設定 system prompt
SYSTEM """
你是一個專業的程式助手，請用繁體中文回答。
"""

# 調整參數
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
```

### 常用 PARAMETER 參數

| 參數 | 說明 | 預設值 |
|------|------|--------|
| `temperature` | 創意度（越高越隨機） | 0.8 |
| `top_p` | 取樣範圍 | 0.9 |
| `top_k` | 取樣候選數 | 40 |
| `num_ctx` | 上下文長度 | 2048 |
| `repeat_penalty` | 重複懲罰 | 1.1 |

### 建立自訂模型

```bash
# 從 Modelfile 建立模型
ollama create my-assistant -f ./Modelfile
# transferring model data
# creating model layer
# writing manifest
# success

# 驗證
ollama list
# NAME              ID            SIZE      MODIFIED
# my-assistant      a]b2c3d4e5    4.7 GB    just now
# llama3            f6g7h8i9j0    4.7 GB    2 days ago

# 執行
ollama run my-assistant
# >>> 輸入訊息開始對話...
```

---

## 儲存對話為新模型

另一種建立自訂模型的方式：在對話中直接儲存。

```bash
# 進入對話
ollama run llama3
# >>>

# 設定 system prompt 或對話後，使用 /save 儲存
>>> /save my-custom-model
# Created new model 'my-custom-model'

# 驗證
ollama list
# NAME              ID            SIZE      MODIFIED
# my-custom-model   x1y2z3a4b5    4.7 GB    just now
# llama3            f6g7h8i9j0    4.7 GB    2 days ago

# 之後可直接使用
ollama run my-custom-model
# >>> 輸入訊息開始對話...
```

---

## RAG 應用

Modelfile 可搭配 RAG（檢索增強生成）使用，透過 SYSTEM prompt 定義檢索行為。

### RAG 用 Modelfile 範例

```dockerfile
FROM llama3

SYSTEM """
你是一個知識庫助手。請根據提供的上下文資料回答問題。
規則：
1. 只使用提供的資料回答
2. 如果資料中沒有相關內容，請誠實說明「資料中未提及」
3. 回答時引用資料來源
"""

PARAMETER temperature 0.3
PARAMETER top_p 0.9
```

---

## 模型大小計算

### 參數量（B = Billion）

模型名稱中的 `7b`、`8b`、`13b`、`70b` 代表參數量（十億）。

### 計算公式

```
模型大小 = 參數量(B) × 每參數位元組數
```

### 精度對應位元組數

| 精度 | 每參數位元組數 | 說明 |
|------|----------------|------|
| FP32 | 4 bytes | 全精度浮點數 |
| FP16 / BF16 | 2 bytes | 半精度浮點數 |
| INT8 (Q8) | 1 byte | 8-bit 量化 |
| INT4 (Q4) | 0.5 byte | 4-bit 量化 |

> **為什麼大部分模型都用 FP16？**
> 根據研究指出，FP16 相較於 FP32，模型效能損失的幅度非常小（約 0.01%），但可以換來成倍的推理速度和減半的模型體積。這也是目前大部分模型主要都提供 FP16 的原因。

### FP16 vs BF16

**BF16 = Brain Floating Point 16**，由 Google Brain 團隊開發。

| 格式 | 結構 | 特點 |
|------|------|------|
| FP32 | 1 符號 + 8 指數 + 23 尾數 | 全精度 |
| FP16 | 1 符號 + 5 指數 + 10 尾數 | 精度較高，動態範圍較小 |
| BF16 | 1 符號 + 8 指數 + 7 尾數 | 精度較低，動態範圍與 FP32 相同 |

BF16 保留了 FP32 的指數位數，數值範圍一樣大，訓練時不容易溢出(更穩定)，但精度比 FP16 低。

### MXFP4（Microscaling FP4）

由 Microsoft、NVIDIA、ARM、Intel 等大廠聯合推出的 4-bit 浮點格式（OCP 標準）。

**與 INT4 的差異：**

| 格式 | 類型 | 結構 | 特點 |
|------|------|------|------|
| INT4 | 整數 | 4-bit 整數 | 範圍固定，簡單 |
| MXFP4 | 浮點 | 1 符號 + 2 指數 + 1 尾數 | 動態範圍更大 |

**MXFP4 的關鍵：Block Scaling（塊級縮放）**

```
傳統：每個數值獨立存指數
MXFP4：一組數值（如 32 個）共享一個 8-bit scale

┌─────────────────────────────────┐
│ 32 個 4-bit 數值 │ 1 個 8-bit scale │  ← 共享縮放因子
└─────────────────────────────────┘
```

**優點：**
- 比 INT4 保留更多精度（浮點 vs 整數）
- 比傳統 FP4 更省空間（共享 scale）
- 新硬體（NVIDIA Blackwell 等）原生支援

**精度比較：**

| 格式 | 相對 FP16 精度損失 | 體積 |
|------|-------------------|------|
| FP16 | 基準 | 2 bytes |
| INT8 | ~0.5% | 1 byte |
| INT4 | ~5-10% | 0.5 byte |
| MXFP4 | ~1-3% | ~0.5 byte |

> MXFP4 用 INT4 的體積，達到接近 INT8 的精度。

**未來趨勢預測：**

```
現在主流          過渡期            未來
FP16 / INT8  →  INT4 / Q4_K_M  →  MXFP4
```

等硬體普及（NVIDIA Blackwell、下一代 AMD/Intel），Ollama 上應該會開始出現 MXFP4 量化的模型。

### 模型大小換算表

| 參數量 | FP32 (×4) | FP16 (×2) | INT8 (×1) | INT4 (×0.5) |
|--------|-----------|-----------|-----------|-------------|
| **7B** | 7 × 4 = **28 GB** | 7 × 2 = **14 GB** | 7 × 1 = **7 GB** | 7 × 0.5 = **3.5 GB** |
| **8B** | 8 × 4 = **32 GB** | 8 × 2 = **16 GB** | 8 × 1 = **8 GB** | 8 × 0.5 = **4 GB** |
| **13B** | 13 × 4 = **52 GB** | 13 × 2 = **26 GB** | 13 × 1 = **13 GB** | 13 × 0.5 = **6.5 GB** |
| **32B** | 32 × 4 = **128 GB** | 32 × 2 = **64 GB** | 32 × 1 = **32 GB** | 32 × 0.5 = **16 GB** |
| **70B** | 70 × 4 = **280 GB** | 70 × 2 = **140 GB** | 70 × 1 = **70 GB** | 70 × 0.5 = **35 GB** |

> **注意**：實際大小會因模型架構、量化方法略有差異，上表為理論估算值。

### K-Quant 量化版本

| 版本 | 說明 |
|------|------|
| K_S | Small，體積最小，品質稍差 |
| **K_M** | **Medium，官方推薦，平衡首選** |
| K_L | Large，品質較好，體積較大 |

### 模型名稱範例

```
llama3.1:8b-instruct-q4_K_M
│       │  │         │  └─ 量化版本（K_M 推薦）
│       │  │         └──── 量化精度（q4 = 4-bit）
│       │  └────────────── 模型類型（instruct = 指令微調版）
│       └───────────────── 參數量（8B = 80億參數）
└───────────────────────── 模型名稱
```

**常見範例：**
| 名稱 | 說明 |
|------|------|
| `llama3.1:8b` | 8B 預設版本 |
| `llama3.1:8b-instruct-q4_K_M` | 8B 指令版 4-bit K_M 量化 |
| `llama3.1:70b-text-q8_0` | 70B 文字版 8-bit 量化 |
| `qwen2:7b-instruct-q4_K_M` | Qwen2 7B 指令版 4-bit K_M |

---

## 模型壓縮方法

除了量化，還有其他壓縮方式：

### 剪枝（Pruning）

把不重要的權重或神經元直接移除。

| 類型 | 做法 | 特點 |
|------|------|------|
| 非結構化 | 移除單個權重（設為 0） | 矩陣變稀疏，需特殊硬體加速 |
| 結構化 | 移除整個神經元/層 | 直接變小，一般硬體可加速 |

**流程：**
1. 訓練完整模型
2. 評估每個權重的重要性（看絕對值大小、梯度等）
3. 砍掉不重要的
4. 微調恢復精度

### 低秩近似（Low-Rank Approximation）

把大矩陣拆成兩個小矩陣相乘。

```
原始                      低秩近似
W (m × n)        ≈        A (m × r) × B (r × n)

參數量：m × n    →        m × r + r × n
```

**範例：**
- 原始：1000 × 1000 = **100 萬**參數
- 低秩（r=10）：1000×10 + 10×1000 = **2 萬**參數
- 壓縮 50 倍

> **LoRA** 就是這個概念：微調時不改原始權重 W，只訓練低秩的 A×B，參數量大幅減少。

### 蒸餾（Knowledge Distillation）

讓小模型學習大模型的輸出行為。

```
Input → 大模型（Teacher） → 輸出（含機率分布）
                              ↓
              小模型（Student）學習模仿
```

**關鍵：** 小模型不只學最終答案，還學大模型輸出的**機率分布**。例如大模型說「80% 是 A、15% 是 B、5% 是 C」，小模型學整個分布，能學到更多"知識"。

### 壓縮方法比較

| 方法 | 做法 | 優點 | 缺點 |
|------|------|------|------|
| 量化 | 降低數值精度 | 簡單直接 | 精度損失 |
| 剪枝 | 移除不重要權重 | 保留架構 | 需重新訓練 |
| 低秩近似 | 矩陣分解 | 大幅減參數 | 表達能力受限 |
| 蒸餾 | 小模型學大模型 | 效果好 | 需要訓練資源 |

---

## MoE（Mixture of Experts）

一種模型架構，推理時只激活部分「專家」網路，而非整個模型。

```
輸入 → Router（路由器）決定用哪些專家
            ↓
    ┌───┬───┬───┬───┬───┬───┬───┬───┐
    │ E1│ E2│ E3│ E4│ E5│ E6│ E7│ E8│  ← 8 個專家
    └───┴───┴───┴───┴───┴───┴───┴───┘
            ↓ 只激活 2 個
         ┌───┬───┐
         │ E2│ E5│  ← 實際運算
         └───┴───┘
            ↓
          輸出
```

**範例：Mixtral 8x7B**
- 總參數：8 × 7B = **56B**（模型檔案大小）
- 每次激活：2 × 7B = **14B**（實際推理計算量）
- 好處：用 14B 的速度，得到接近 56B 的效果

| 模型 | 總參數 | 激活參數 | 說明 |
|------|--------|----------|------|
| Mixtral 8x7B | 46.7B | ~12.9B | 每次激活 2/8 專家 |
| Mixtral 8x22B | 141B | ~39B | 每次激活 2/8 專家 |

---

## Ollama CPU/GPU 混用

當 VRAM 不足時，Ollama 會自動將部分層放到 CPU 運行。

```
模型層分配（VRAM 不足時）：

GPU（VRAM）    CPU（RAM）
┌─────────┐   ┌─────────┐
│ Layer 1 │   │ Layer 5 │
│ Layer 2 │   │ Layer 6 │
│ Layer 3 │   │ Layer 7 │
│ Layer 4 │   │ Layer 8 │
└─────────┘   └─────────┘
   快 🚀          慢 🐢
```

**速度差異顯著：**

| 運行方式 | 速度 | 說明 |
|----------|------|------|
| 全 GPU | 🚀🚀🚀 最快 | 模型完全載入 VRAM |
| GPU + CPU 混用 | 🚀 中等 | 部分層在 CPU，會拖慢整體速度 |
| 全 CPU | 🐢 最慢 | 沒有 GPU 或 VRAM 完全不足 |

> **建議：** 盡量選擇能完全載入 VRAM 的模型大小/量化版本，避免混用帶來的性能損失。
