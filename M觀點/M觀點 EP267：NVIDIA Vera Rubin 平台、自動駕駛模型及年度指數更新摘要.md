# M觀點 EP267：NVIDIA Vera Rubin 平台、自動駕駛模型及年度指數更新摘要

## 執行摘要

本文件綜合分析了 M 觀點第 267 集直播的內容，核心聚焦於 NVIDIA 在 CES 大會上發布的重大技術進展以及 M 觀點的年度投資指數更新。主要洞察如下：

1. **NVIDIA Vera Rubin 新一代平台：** NVIDIA 正式揭露其 Blackwell 後的下一代 AI 基礎設施平台，名為 Vera Rubin。該平台包含 Vera CPU 與 Rubin GPU。Rubin GPU 的效能提升尤為顯著，其核心突破在於「硬體動態壓縮」技術，使得在 NVFP4 精度下，訓練效能達到 Blackwell 的 3.5 倍，推論效能更高達 5 倍。此技術的實現，遠超純粹由電晶體數量增加所帶來的 60-70% 基礎效能增長。
2. **NVIDIA "Drive Concierge" 自動駕駛模型：** NVIDIA 發布名為 "Drive Concierge" 的開源自動駕駛「教師模型」，旨在協助歐美傳統車廠追趕在自動駕駛領域的進度，特別是相對於特斯拉與中國車廠。此模型採用視覺語言行動（VLA）架構，具備較佳的可解釋性。它的推出被視為驗證了特斯拉從 Level 2 系統逐步升級收集數據的發展路線是正確的，並為其他車廠提供了類似 Meta Llama 之於大型語言模型的基礎框架。
3. **M 觀點 2026 年度指數更新：** 報告公布了 2025 年度 EDT10 與 TJ15 指數的表現，分別錄得 25.9% 與 30.3% 的年增長率，均優於 S&P 500 (17.7%) 與 Nasdaq-100 (20.8%) 的表現。針對 2026 年，指數進行了調整：EDT10 將印度 ETF 替換為越南 ETF；TJ15 的前五大權重股中，以 Google 取代 Amazon，並在其他持股中納入 Broadcom (AVGO) 與 Cloudflare (NET)。

\--------------------------------------------------------------------------------

## 1. NVIDIA 新一代 AI 平台：Vera Rubin 深度解析

NVIDIA 於 CES 的主題演講中，正式發表了繼 Blackwell 之後的下一代 AI 平台——Vera Rubin。此平台由新一代的 Vera CPU 與 Rubin GPU 構成，展現了 NVIDIA 作為一個完整 AI 系統架構設計商的領導地位，而非單純的晶片供應商。

### Vera CPU

- **效能躍進**：相較於前代 Grace CPU，Vera CPU 的效能提升了兩倍。
- **架構升級**：核心數增加，並引入了「空間多執行緒」（Spatial Multi-threading）技術，允許每個核心處理兩個不會互相搶佔資源的獨立執行緒，大幅增強處理能力。
- **升級動機**：CPU 的升級是為了匹配 Rubin GPU 的強大算力，確保能有效率地為 GPU 提供數據，避免產生系統瓶頸。

### Rubin GPU

- **製程與時程**：採用台積電 3 奈米製程。目前已進入量產階段，預計 2025 年第三季開始小量出貨，第四季實現大量出貨。
- **命名慣例**：新一代的機櫃解決方案正式定名為 Rubin NVL72，與前代 Blackwell GB200 NVL72 保持一致的命名邏輯（代表一個機櫃內含 72 顆 GPU），揚棄了先前曾提及的 NVL144（以 die 的數量計算）稱謂，以避免市場混淆。

#### 效能解密：從 1.6 倍到 5 倍的飛躍

Rubin GPU 的效能提升幅度是一個複雜議題，其關鍵在於是否採用了 NVIDIA 的新技術。

| 比較基準       | 精度            | 關鍵技術     | Rubin vs. Blackwell 效能倍數 | 備註                                                         |
| -------------- | --------------- | ------------ | ---------------------------- | ------------------------------------------------------------ |
| **純硬體算力** | FP16 (稠密模式) | 無           | **1.6 倍** (增加 60%)        | 此數據反映了電晶體與運算線路增加所帶來的基礎效能提升。       |
| **AI 訓練**    | NVFP4           | 硬體動態壓縮 | **3.5 倍**                   | 該技術能自動將稠密模式的運算轉換為稀疏模式的效率，使效能直接翻倍。 |
| **AI 推論**    | NVFP4           | 硬體動態壓縮 | **5.0 倍**                   | 推論任務對精度的要求更低，動態壓縮效率更高，帶來額外的效能增益。 |

**核心技術突破：硬體動態壓縮 (Hardware Dynamic Compression)**

此為 Rubin GPU 實現效能大幅超越的關鍵。它允許 GPU 在硬體層級動態調整運算精度，能將以「稠密模式」設計的 AI 模型，自動以「稀疏模式」的高效率進行運算，而無需開發者修改模型。這項創新使得稀疏模式的理論算力優勢（通常為稠密模式的兩倍）得以在現實應用中普及，從而將基礎的 1.6-1.7 倍硬體提升放大至 3.5 至 5 倍。

#### 成本與效率優化

- **推論成本**：在 AI 推論任務中，Rubin NVL72 系統處理每個 token 的成本僅為 Blackwell 系統的 **十分之一**。
- **散熱技術**：系統散熱不再需要專門的冷卻基礎設施，僅需使用 45°C 的溫水即可，大幅降低能源消耗。
- **共享上下文記憶體 (Inference Context Memory)**：此技術允許系統將常用資料（如熱門財報文件）處理後的 KVCache 儲存於 SSD 中。當其他使用者上傳相同文件時，系統可直接調用已儲存的 KVCache，無需重複運算，從而：
  - **提升速度**：減少首次處理延遲。
  - **降低運算負載**：釋放 GPU 算力。
  - **節省記憶體**：減少高頻寬記憶體（HBM）的佔用。

## 2. NVIDIA 自動駕駛模型 "Drive Concierge"

NVIDIA 推出名為 "Drive Concierge" 的開源自動駕駛模型，其定位類似於大型語言模型領域中的 Meta Llama，旨在為缺乏自研能力的歐美傳統車廠提供一個強大的基礎模型，以應對來自特斯拉及中國車廠的競爭。

### 模型定位與策略

- **教師模型 (Teacher Model)**：此模型規模龐大，運行於雲端。車廠可利用此教師模型，結合自身收集的駕駛數據進行微調（Fine-tuning），訓練出適合部署在車輛上的輕量化「學生模型」。
- **開源模式**：允許任何車廠基於此模型進行客製化開發，例如賓士（Mercedes-Benz）已宣佈合作，未來可利用其車隊收集的數據，訓練出獨有的 Drive Concierge 版本。
- **市場目標**：主要目的在於協助歐美車廠，使其在自動駕駛技術上不致於完全落後於中國競爭對手。

### 技術路線分析

#### 1. 驗證特斯拉的發展路徑

Drive Concierge 的推出，間接證實了特斯拉從 Level 2 輔助駕駛系統開始，透過大規模量產車輛收集真實世界影像數據，再逐步迭代升級至 Level 4/5 全自動駕駛的策略是成功的。傳統車廠若要追趕，必須效仿此模式，在量產車型上標配足夠的攝影機，先從 Level 2+ 系統開始積累數據。

#### 2. VLA vs. 端到端 (End-to-End) 模型比較

| 特性         | NVIDIA Drive Concierge (VLA 模型)                            | Tesla FSD (端到端神經網路)                                   |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **架構**     | 視覺-語言-行動 (Vision-Language-Action)。由多個協作的神經網路組成（感知、語言推理、行動決策）。 | 單一、巨大的神經網路。影像輸入，駕駛指令直接輸出。           |
| **運作方式** | 影像感知後，生成語言形式的推理鏈（Chain of Thought），再交由行動模組執行。 | 類似人類老司機的直覺反應，決策過程為黑箱。                   |
| **優點**     | **可解釋性強**：由於中間有語言推理步驟，系統的決策過程易於理解和追溯。 | **效能上限高**：資訊在單一網路中傳遞，沒有中間轉換的損耗，理論上能達到更高的駕駛水平。 |
| **缺點**     | **資訊損耗**：將視覺資訊轉換為語言描述的過程中，可能遺失細微但關鍵的細節，影響最終決策的精確度。 | **可解釋性較弱**：決策過程不透明，需透過分析隱含向量等間接方式來進行事後研究。 |
| **發展預期** | 在數據量較少時可能表現更佳。預計在 2029 年左右，其水平或能追上 2026 年的 FSD。 | 在擁有海量數據的情況下，其效能潛力更大，目前領先約 3-5 年。  |

## 3. M 觀點 2026 年度投資指數更新

該節目回顧了其編制的兩大模擬投資指數在 2025 年的表現，並公布了 2026 年的新成分股名單。

### 2025 年度績效回顧

| 指數                | 2025 年報酬率 | S&P 500 (SPY) | Nasdaq-100 (QQQ) |
| ------------------- | ------------- | ------------- | ---------------- |
| **EDT10 (簡單十)**  | **+25.9%**    | +17.7%        | +20.8%           |
| **TJ15 (科技十五)** | **+30.3%**    | +17.7%        | +20.8%           |

兩項指數在 2025 年的表現均顯著超越了市場主要指數。

### 2026 年度指數調整

#### EDT10 (簡單十)

此指數旨在提供一個簡單的長線投資組合，由 10 個標的平均分配資金。

- **調整內容**：
  - **移除**：印度 ETF (INDA)。理由是印度在地緣政治中採取搖擺策略，可能不會成為美國主力扶植的對象。
  - **加入**：越南 ETF (VNM)。理由是越南在美中貿易戰中處理得當，有望持續承接從中國轉移的供應鏈。
- **2026 年完整名單**：
  1. Microsoft (MSFT)
  2. NVIDIA (NVDA)
  3. Google (GOOGL)
  4. Tesla (TSLA)
  5. Meta Platforms (META)
  6. Apple (AAPL)
  7. Amazon (AMZN)
  8. 半導體 ETF (SMH)
  9. AI 與機器人 ETF (ARKQ)
  10. **越南 ETF (VNM)**

#### TJ15 (科技十五)

此指數代表講者最看好的 15 家科技公司，前 5 家權重各 10%，後 10 家權重各 5%。

- **權重 10% 的前五大持股調整**：
  - **移除**：Amazon (AMZN)。理由是在 AI 領域的進展相對其他巨頭稍慢。
  - **加入**：Google (GOOGL)。
  - **新名單**：Microsoft, NVIDIA, **Google**, Tesla, Meta.
- **權重 5% 的後十檔持股調整**：
  - **移除**：Shopify (SHOP), Booking Holdings (BKNG).
  - **加入**：Broadcom (AVGO), Cloudflare (NET).
- **2026 年完整名單**：
  - **10% 權重**：Microsoft, NVIDIA, Google, Tesla, Meta.
  - **5% 權重**：Apple, Netflix, TSMC (TSM), **Broadcom (AVGO)**, Palantir (PLTR), SEA Ltd (SE), Snowflake (SNOW), CrowdStrike (CRWD), **Cloudflare (NET)**, ARKQ ETF.